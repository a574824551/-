人脸DeepFake:

Face2Face表情迁移：该方法借助Dlib，首先对图片中的人脸进行检测，找到人脸上的关键标记点，然后使用针对人脸的 pix2pix转换模型把关键标记点转换为目标人脸图像，实现了从源视频到目标视频的实时且高度逼真的面部表情迁移
THIES J, ZOLLHOFER M, STAMMINGER M, et al. Face2face: Real-time face capture and reenactment of rgb videos[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2387-2395.

HeadOn②在Face2Face的基础上进行了改进，通过多个神经网络，让表情细节变得更加自然，如表情凝视、头部移动等
THIES J, ZOLLHÖFER M, THEOBALT C, et al. Headon: Real-time reenactment of human portrait videos[J]. ACM Transactions on Graphics (TOG), 2018, 37(4): 1-13.

相似的工作还有Kim等人③提出的利用时空架构的生成网络，将合成的渲染图片转换为真实图，进行脸部表情的迁移。
KIM H, GARRIDO P, TEWARI A, et al. Deep video portraits[J]. ACM Transactions on Graphics (TOG), 2018, 37(4): 1-14.

Suwajanakorn等人④将RNN应用到嘴型动作重建中，可以根据输入语音合成符合发音的嘴型动作。不同场景的表情迁移技术日益成熟。
SUWAJANAKORN S, SEITZ S M, KEMELMACHER-SHLIZERMAN I. Synthesizing obama: learning lip sync from audio[J]. ACM Transactions on Graphics (ToG), 2017, 36(4): 1-13.

FaceSwap是一种基于人脸融合的人脸替换方法。它首先获取人脸关键点，然后通过3D模型对人脸关键点位置进行建模，基于目标人物表情对替换人脸进行渲染，最后将渲染得到的人脸通过图像处理等操作融合到目标人物人脸上。图7.8展示了一种基于FaceSwap的改进后的人脸融合方法，为了促进最终的融合过程，该方法使用人脸分割算法对人脸区域进行了分割，仅在分割后的人脸区域进行人脸融合操作。Nirkin等人提出用分割的思路促进换脸，通过网络分割出来的人脸估计3D人脸形状，最后融合源和目标这两个对齐的3D人脸形状。
FaceSwap: 引自GitHub中的FaceSwap项目，2021/06/05.

基于人脸融合的换脸方法通常成本较低，只需要一张替换目标的人脸照片，但针对人脸角度变化较大的场景，往往合成效果较差。因此研究人员开始关注将深度学习技术应用到人脸生成中，这类方法多采用自动编码器（Autoencoder）或生成对抗网络（GAN）。

Deepfakes是较早开源的基于自动编码器的换脸网络，它的整体流程分为训练阶段和生成阶段两个部分。自动编码器往往由一个编码器和一个解码器组成，其中编码器对输入进行降维，解码器使用降维后的变量来得到与输入相似的输出。基于自动编/解码器，通过用源人物和目标人物的几百张照片训练模型分别识别、还原两人面部的能力，最后用源人物的照片搭配目标人物的解码器就可以完成转换。Deepfakes的局限性在于，它往往需要上百张甚至更多的样本来进行训练，训练过程往往消耗大量时间和资源。

GAN被广泛用于换脸中，FaceSwap-GAN①是增加了GAN技术的Deepfakes，引入鉴别器损失函数，在生成的过程中判断生成的图像和原始图像的相似度，来提升生成图像的质量。CycleGAN②③利用GAN学习两个类别之间的转换关系的特点，在不需要成对数据的前提下实现了不同的两个域之间的图像转换。该方法可以视为换脸技术的早期尝试，但并未达到很好的换脸效果。后期提出的ReCycleGAN④⑤，结合了空间信息和视频流时间信息，并结合了内容转换和风格保留的对抗损失。相比于CycleGAN，ReCycleGAN在细节上更拟合目标图像。GAN的加入使得生成的人脸更加逼真自然。除换脸外，GAN还被广泛用于生成虚拟的人脸和篡改人脸属性，如starGAN⑥、StackGAN⑦、PGAN⑧等。
① FaceSwap-GAN: 引自GitHub官网shaoanlu/faceswap-GAN库，2021/06/05.
② ZHU J Y, PARK T, ISOLA P, et al. Unpaired image-to-image translation using cycle-consistent adversarial networks [C]//Proceedings of the IEEE international conference on computer vision. 2017: 2223-2232.
③ CycleGAN: 引自GitHub官网错误!超链接引用无效。 库，2021/06/05.
④ BANSAL A, MA S, RAMANAN D, et al. Recycle-gan: Unsupervised video retargeting[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 119-135.
⑤ ReCycleGAN: 引自GitHub官网SunnerLi/RecycleGAN库，2021/06/05.
⑥ CHOI Y, CHOI M, KIM M, et al. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 8789-8797.
⑦ ZHANG H, XU T, LI H, et al. Stackgan++: Realistic image synthesis with stacked generative adversarial networks[J]. IEEE transactions on pattern analysis and machine intelligence, 2018, 41(8): 1947-1962.
⑧ KARRAS T, AILA T, LAINE S, et al. Progressive growing of gans for improved quality, stability, and variation[J]. arXiv preprint arXiv:1710.10196, 2017.


语音DeepFake:

随着深度学习技术的发展，TTS、语音转换、语音克隆技术都得到了大幅度的提升。谷歌提出了第一个端到端的语音合成算法WaveNet①，可以合成与人相似的音频。类似的TTS模型还有Tacotron②和DeepVoice③，后来Arik等人和Ping等人分别对DeepVoice系列进行了改进，提出了DeepVoice2①和DeepVoice3②。其中，DeepVoice2通过低维度的可训练的说话者编码来增强文本到语音的转换，使得单个模型能够生成不同的语音；而DeepVoice3是一个基于注意力机制的全卷积TTS系统，加快了语音合成的速度。
① OORD A, DIELEMAN S, ZEN H, et al. Wavenet: A generative model for raw audio[J]. arXiv preprint arXiv: 1609.03499, 2016.
② WANG Y, SKERRY-RYAN R J, STANTON D,et al. Tacotron: Towards end-to-end speech synthesis[C]//Interspeech 2017, 18th Annual Conference of the International Speech Communication Association.2017: 4006-4010.
③ ARIK S, CHRZANOWSKI M, COATES A, et al.Deep voice: Real-time neural text-to-speech[C]// 34th International Conference on Machine Learning. 2017: 195-204.

与图像伪造技术类似，GAN时常被用于语音伪造中的语音转换中，也就是将源人物的语音转换为目标人物的语音。例如，Kaneko等人③利用 GAN 的一种特殊架构CycleGAN进行了语音转换。CycleGAN这种语音转换方法需要事先指定源说话人和目标说话人的身份，而Kinnunen等人④借鉴了说话识别中的Ivector与PLDA，打破这种局限，只训练一个系统，就能处理许多源说话人和目标说话人。自编码器也被用于语音转换中，模型中包含一个编码器和一个解码器，编码器负责把数据的表层特征进行隐表示，解码器负责从隐表示中恢复出表层特征。在语音转换任务中，数据的表层特征可以是波形、语谱图、MFCC序列等；隐表示则蕴含了语音的内容和说话人的身份信息。基于自编码器的语音转换工作包括论文⑤等。
① ARIK S, DIAMOS G, GIBIANSKY A, et al. Deep voice 2: Multi-speaker neural text-to-speech[C]//Advances in neural information processing systems. 2017: 2962-2970.
② PING W, PENG K, GIBIANSKY A, et al. Deep voice 3: 2000-speaker neural text-to-speech[J]. Proc. ICLR, 2018: 214-217.
③ KANEKO T, KAMEOKA H. Parallel-data-free voice conversion using cycle-consistent adversarial networks[J]. arXiv preprint arXiv:1711.11293, 2017.
④ KINNUNEN T, JUVELA L, ALKU P, et al. Non-parallel voice conversion using i-vector PLDA: Towards unifying speaker verification and transformation[C]//2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017: 5535-5539.
⑤ HSU C C, HWANG H T, WU Y C, et al. Voice conversion from non-parallel corpora using variational auto-encoder [C]//2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). IEEE, 2016: 1-6.

与前面介绍的文本转语音的TTS技术、语音转语音的语音转换技术不同，语音克隆技术的输入往往既有文本内容，又有语音内容。语音克隆系统根据输入的语音内容来提取说话人的音色特征，根据输入的文本内容来控制生成的语音需要包含的内容，可以让目标人物“说”任何想“说”的话。其实这项技术也可以视为一个多说话人的TTS系统。Jia等人①提出了一个经典的语音克隆系统，该系统包含一个语音编码器，用于提取说话人的音色特征；一个文本编码器，用于编码需要生成的文本内容。得到语音编码和文本编码后将它们进行拼接，送入一个解码器中生成梅尔声谱图，最后连接一个声码器将梅尔声谱图合成最终的语音，整体流程如图7.11所示。
① JIA Y, ZHANG Y, WEISS R, et al. Transfer learning from speaker verification to multispeaker text-to-speech synthesis[J]. Advances in neural information processing systems, 2018.



常见的深度伪造数据集：
数据类型	数据集	真实视频（语音）数量/伪造视频（语音）数量（个）
视觉伪造数据	UADFV	49/49
	Celeb-DF	590/5639
	FaceForensics	1004/1004
	FaceForensics++	1000/1000
	Deepfake-TIMIT	320/640
	Mesonet data	11509/8000
	DFDC	100129/19025
	DeepForensics-1.0 	50000/10000
	WildDeepfake 	0/1869
语音伪造数据	ASVspoof 2015	9404/184000
	ASVspoof 2019	-

