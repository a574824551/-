美国高级研究计划署（IARPA）资助了一项对AI后门的研究项目，为期两年。这里简单记录一下笔记和思考。
https://www.iarpa.gov/research-programs/trojai

定义上，他们锚定的后门攻击更具真实意义，对于像素空间上用几个特征组成的像素触发器并不是特别在意，更在意在真实世界、特征空间中添加触发器能够实现后门攻击。例如一个红色八角形且中间有黄色的图案出现在任意一个角落都会导致后门的触发。

不讨论现实意义不太的，比如修改数据集、白盒攻击、暴力搜索、直接编辑权重等等。

方案一：数据集检测，离群数据有可能是后门攻击。（离群点也有可能数据集质量不高，这种方法得预先知道数据集中有触发器，不然意义有限，不是一种好的通用方法

方案二：调整输入直到错误识别，逐步修改正常输入样本直到模型判断错误，测试修改的大小是否非常小。如果极其小有可能是木马触发器。（one-pixel攻击等虽然概率小，但也不能忽视，尤其是这种迭代式的攻击方法，误差率很高。

方案三：检查木马是否被触发。检查输入是否会让模型对特定地方过分关注。（检测时间未知，作为事后溯源可能更靠谱一些

以项目赞助号可以搜到相关的项目论文：
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=W911NF20C0034+OR+W911NF20C0038+OR+W911NF20C0045+OR+W911NF20C0035+OR+IARPA-20001-D2020-2007180011

https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-90.pdf
这一篇通过对抗样本寻找触发器，对每一类样本都去做对抗攻击，看看能不能找到某个通用的扰动对某一类高度敏感，那就有可能是后门触发器。当然这也要对触发器的大小进行不断的尝试、约束。是个有效的方法，但是计算成本很高。

